{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In these three exercises you will get an introduction to the functionality of the new pyRSA-toolbox. For illustration these exercises use simulated RDMs from the paper \"Inferring brain-computational mechanisms with models of activity measurements\" by Kriegeskorte & Diedrichsen (2016). These data are generated by applying different measurement models to the layers of Alexnet, the deep neural network model, which sparked the interest in deep learning. Thus, we have noise free model RDMs for each layer, which represent different measurement models, i.e. models how the representation is distorted by the measurement. Additionally, we have simulated fMRI data with different amounts of noise added. \n",
    "\n",
    "Our overall aim in this setting is to infer which representation the data rdms were based on, i.e. which layer was used for generating the data. Towards this aim we will make three steps:\n",
    "\n",
    "In *Exercise 1*, we will load the data, convert them into the formats used in the toolbox and have a first exploratory look at the data.\n",
    "\n",
    "In *Exercise 2*, we will compare the RDMs based on the undistorted representations to the measured RDMs. This is the classical and simplest approach and already allows us to perform model comparisons and the general evaluation of model-RDMs. We will see that this does not allow us to correctly infer the underlying representation, because the measurement process distorts the RDMs too much.\n",
    "\n",
    "In *Exercise 3*, we will apply flexible models, which allow the models to choose the measurement model which best explains the data. To evaluate such flexible models additional cross-validation is necessary, which we also discuss in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Data and RDM handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pyrsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model RDMs\n",
    "Here the models are different layers of Alexnet.\n",
    "For each layer, different models of how the fMRI voxels sample the neurons are being considered.\n",
    "\n",
    "The simulated data were generated in Matlab (Kriegeskorte & Diedrichsen 2016). Thus, we load the Matlab files in .mat format.\n",
    "\n",
    "For each model-RDM, we obtain a model name which is the layer used to generate RDM, a measurement model, which specifies the applied distortions and the actual rdm, which was saved as a vector for each RDM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_data = io.matlab.loadmat('rdms_inferring/modelRDMs.mat')\n",
    "matlab_data = matlab_data['modelRDMs']\n",
    "n_models = len(matlab_data[0])\n",
    "model_names = [matlab_data[0][i][0][0] for i in range(n_models)]\n",
    "measurement_model = [matlab_data[0][i][1][0] for i in range(n_models)]\n",
    "rdms_array = np.array([matlab_data[0][i][2][0] for i in range(n_models)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are not specific to the toolbox, but to the format the RDMs were originally saved in.\n",
    "To load other data, simply transform them such that they are numpy arrays of either the whole RDM or vector format of the upper triangular part of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the model RDMs as a pyRSA object\n",
    "We place the RDMs in a pyRSA object which can contain additional descriptors for the RDMs and the experimental conditions.\n",
    "Here we label each RDM with the name of the brain-computational model (AlexNet layer) and the name of the measurement model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rdms = pyrsa.rdm.RDMs(rdms_array,\n",
    "                            rdm_descriptors={'brain_computational_model':model_names,\n",
    "                                             'measurement_model':measurement_model}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_rdms is now a custom object, which contains all the rdms from the .mat file with the additional information.\n",
    "It also has a few functions for forming subsets of the data, saving and loading, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the RDMs from AlexNet layer conv1\n",
    "\n",
    "As a simple example select the rdms, which correspond to the first convolutional layer. These can then be plotted using the function pyrsa.vis.show_rdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_rdms = model_rdms.subset('brain_computational_model','conv1')\n",
    "plt.figure(figsize=(10,10))\n",
    "pyrsa.vis.show_rdm(conv1_rdms, do_rank_transform=True, rdm_descriptor='measurement_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the RDMs which were generated from convolutional layer 1 by different measurement models. Each RDM is labeled with the name of the measurement model. Also in the lower right corner the average RDM is plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print info about a set of RDMs\n",
    "The pyRSA objects can simply be passed to the print function to obtain a short description of their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv1_rdms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Of course, you can also show all RDMs or select any other subset. Have a look at the different RDMs!\n",
    "\n",
    "How many RDMs are there for each layer?\n",
    "\n",
    "Generate a plot which shows all RDMs with the 'complete' measurement model.\n",
    "\n",
    "How different do the different measurement models look to you and how different do the different layers look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Fixed model inference\n",
    "## Load data RDMs\n",
    "Here we use simulated data to demonstrate RSA inference.\n",
    "Since we know the true data-generating model in each case, we can tell when inference fails or succeeds.\n",
    "\n",
    "For each data RDM, we obtain the name of the underlying Layer, a full width half maximum (fhwm) value specifying how strongly the simulated voxels average the representation and a noise standard deviation, specifying how much noise was added to the voxel responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_data = io.matlab.loadmat('rdms_inferring/noisyModelRDMs_demo.mat')\n",
    "repr_names_matlab = matlab_data['reprNames']\n",
    "fwhms_matlab = matlab_data['FWHMs']\n",
    "noise_std_matlab = matlab_data['relNoiseStds']\n",
    "rdms_matlab = matlab_data['noisyModelRDMs']\n",
    "repr_names = [repr_names_matlab[i][0][0] for i in range(repr_names_matlab.shape[0])]\n",
    "fwhms = fwhms_matlab.squeeze().astype('float')\n",
    "noise_std = noise_std_matlab.squeeze().astype('float')\n",
    "rdms_matrix = rdms_matlab.squeeze().astype('float')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose one of the data RDMs for inference\n",
    "\n",
    "Here we choose which data RDMs we use for the exercise. You can change the representation, tthe noise level and the amount of averaging by chaning the index values at the beginning.\n",
    "\n",
    "We then convert the chosen data RDMs into an pyrsa RDMs object and display them as we did for the model RDMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices choosing brain-computational model, noise level, and the size of the kernel with which each voxel samples the neural activity\n",
    "i_rep = 2 #np.random.randint(len(repr_names)) \n",
    "i_noise = 1 #np.random.randint(len(noise_std))\n",
    "i_fwhm = 0 #np.random.randint(len(fwhms))\n",
    "\n",
    "# print the chosen representation definition\n",
    "repr_name = repr_names[i_rep]\n",
    "print('The chosen ground truth model is:')\n",
    "print(repr_name)\n",
    "print('with noise level:')\n",
    "print(noise_std[i_noise])\n",
    "print('with averaging width (full width at half magnitude):')\n",
    "print(fwhms[i_fwhm])\n",
    "\n",
    "# put the rdms into an RDMs object and show it\n",
    "rdms_data = pyrsa.rdm.RDMs(rdms_matrix[:, i_rep, i_fwhm, i_noise, :].transpose())\n",
    "pyrsa.vis.show_rdm(rdms_data, do_rank_transform=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fixed models\n",
    "An \"RDM model\" is a pyRSA object that can predict a data RDM.\n",
    "For example, an RDM model may contain a set of predictor RDMs, which predict the data RDM as a weighted combination.\n",
    "Here we use fixed RDM models, which contain just a single RDM with no parameters to be fitted.\n",
    "\n",
    "Models are generated by first choosing the right RDM in this case the one with the right \"brain_computational_model\" and the \"measurement_model\" \"complete\", which corresponds to no distortions added. This object is then passed to the function pyrsa.model.ModelFixed, which generates a fixed RDM model. These RDM models are then collected in the list models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i_model in np.unique(model_names):\n",
    "    rdm_m = model_rdms.subset('brain_computational_model', i_model).subset('measurement_model','complete')\n",
    "    m = pyrsa.model.ModelFixed(i_model, rdm_m)\n",
    "    models.append(m)\n",
    "\n",
    "print('created the following models:')\n",
    "for i in range(len(models)):\n",
    "    print(models[i].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSA 1.0: Just compare model RDMs to measured RDMs\n",
    "evaluate models naively, i.e. simply compute the average correlation to the data RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "#results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='spearman')\n",
    "#pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "#results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='tau-a')\n",
    "#pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "#results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='rho-a')\n",
    "#pyrsa.vis.plot_model_comparison(results_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these plots the models do not have errorbars as we did not run any estimate of the variablitiy.\n",
    "The upper noise ceiling is computed by finding the RDM with the highest possible average similarity to the measured RDMs. This is not 1 because the RDMs for different subjects or measurements differ. The lower noise ceiling is a leave one out crossvalidation of this averaging procedure, i.e. we find the RDM to perform optimally on all but one of the RDMs and evaluate this average RDM on the left out RDM. Each RDM is left out once.\n",
    "\n",
    "To perform statistical comparisons and estimate how uncertain we should be about the models' performance we can perform bootstrapping:\n",
    "\n",
    "In each plot the errobars correspond to +/- one SEM based on the bootrap samples.\n",
    "The lines above the plot show which pairwise comparisons are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison by bootstrapping the subjects\n",
    "We can bootstrap resample the subjects, which estimates how variable the model performances would be if we sampled new subjects with the same stimuli. Based on that uncertainty estimate we can statistically compare model performances against each other. As we have many comparisons between models we perform FDR correction on those tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2a = pyrsa.inference.eval_bootstrap_rdm(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2a, plot_pair_tests='FDR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison by bootstrapping the stimuli\n",
    "We can alternatively bootstrap resample the stimuli to estimate how much model performance would vary if we sampled the subjects we have with new stimuli. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2b = pyrsa.inference.eval_bootstrap_pattern(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2b, plot_pair_tests='FDR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison by bootstrapping both stimuli and subjects\n",
    "Finally, we can bootstrap resample both stimuli and subjects to estimate how variable the model performances would be if we measured new subjects with new stimuli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results_2c = pyrsa.inference.eval_bootstrap(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2c, plot_pair_tests='FDR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Does the right model win? And do the mean estimates from bootstrapping differ from the evaluations over the whole dataset?\n",
    "\n",
    "Compare the results for the different bootstrapping methods. Which method leads to the widest confidence intervals, which one to the smallest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Crossvalidation for flexible models\n",
    "## defining flexible models\n",
    "Here we use selection models, i.e. each model layer gets a list of rdms and only specifies that one of those is the right one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_flex = []\n",
    "for i_model in np.unique(model_names):\n",
    "    models_flex.append(pyrsa.model.ModelSelect(i_model,\n",
    "        model_rdms.subset('brain_computational_model', i_model)))\n",
    "\n",
    "print('created the following models:')\n",
    "for i in range(len(models_flex)):\n",
    "    print(models_flex[i].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation\n",
    "As we are now using flexible models, we have to do crossvalidation to get an estimate how well the model would do on new unseen data.\n",
    "\n",
    "As a first step we create the training test and ceil sets. The last one corresponds to the training data for the testset which is necessary for calculating a noise ceiling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, ceil_set = pyrsa.inference.sets_k_fold(rdms_data, k_pattern=3, k_rdm=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these sets we can now evaluate our models as we did without crossvalidaton and plot the results. With a single training/testset combination we do not get errorbars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_cv = pyrsa.inference.crossval(models_flex, rdms_data, train_set, test_set,\n",
    "                                        ceil_set=ceil_set, method='corr')\n",
    "# plot results\n",
    "pyrsa.vis.plot_model_comparison(results_3_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapped Crossvalidation\n",
    "\n",
    "Finally, we can perform bootstrapping around the crossvalidation to get uncertainty estimates for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_3_full = pyrsa.inference.bootstrap_crossval(models_flex, rdms_data, k_pattern=3, k_rdm=2, method='corr', N=100)\n",
    "# plot results\n",
    "pyrsa.vis.plot_model_comparison(results_3_full, plot_pair_tests='FDR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the right model win?\n",
    "\n",
    "Try some different settings for the bootstrap: How do the results change when you make the training and testsets larger or smaller?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
